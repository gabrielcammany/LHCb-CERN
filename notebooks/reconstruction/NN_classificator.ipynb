{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from rtree import index\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r mc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "        xlayer_model = load_model('./models/xlayer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   MCParticleID\n",
      "1   MC_Hit_X\n",
      "2   MC_Hit_Y\n",
      "3   MC_Hit_Z\n",
      "4   PrHit_LHCbID\n",
      "5   PrHit_Xat0\n",
      "6   PrHit_Zat0\n",
      "7   PrHit_isX\n",
      "8   PrHit_planeCode\n",
      "9   PrHit_w2\n",
      "10   PrHit_yMax\n",
      "11   PrHit_yMin\n",
      "12   PrHit_zone\n",
      "13   event\n"
     ]
    }
   ],
   "source": [
    "###### Data columns\n",
    "i = 0\n",
    "for val in mc_data.columns:\n",
    "    print(i,\" \",val)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_hits = mc_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r zLays\n",
    "%store -r max_x\n",
    "%store -r min_x\n",
    "%store -r events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15837 15838 15839 15840 15841 15842 15843 15844 15845 15846 15847 15848\n",
      " 15849 15850 15851 15852 15853 15854 15855 15856 15857 15858 15859 15860\n",
      " 15861 15862 15863 15864 15865 15866 15867 15868 15869 15870 15871 15872\n",
      " 15873 15874 15875 15876 15877 15878 15879 15880 15881 15882 15883 15884\n",
      " 15885 15886 15887 15888 15889 15890 15891 15892 15893 15894 15895 15896\n",
      " 15897 15898 15899 15900 15901 15902 15903 15904 15905 15906 15907 15908\n",
      " 15909 15910 15911 15912 15913 15914 15915 15916 15917 15918 15919 15920\n",
      " 15921 15922 15923 15924 15925 15926 15927 15928 15929 15930 15931 15932\n",
      " 15933 15934 15935 15936 15937 15938 15939 15940 15941 17909 17910 17911\n",
      " 17912 17913 17914 17915 17916 17917 17918 17919 17920 17921 17922 17923\n",
      " 17924 17925 17926 17927 17928 17929 17930 17931 17932 17933 17934 17935\n",
      " 17936 17937 17938 17939 17940 17941 17942 17943 17944 17945 17946 17947\n",
      " 17948 17949 17950 17951 17952 17953 17954 17955 17956 17957 17958 17959\n",
      " 17960 17961 17962 17963 17964 17965 17966 17967 17968 17969 17970 17971\n",
      " 17972 17973 17974 17975 17976 17977 17978 17979 17980 17981 17982 17983\n",
      " 17984 17985 17986 17987 17988 18649 18650 18651 18652 18653 18654 18655\n",
      " 18656 18657 18658 18659 18660 18661 18662 18663 18664 18665 18666 18667\n",
      " 18668 18669 18670 18671 18672 18673 18674 18675 18676 18677 18678 18679\n",
      " 18680 18681 18682 18683 18684 18685 18686 18687 18688 18689 18690 18691\n",
      " 18692 18693 18694 18695 18696 18697 18698 18699 18700 18701 18702 18703\n",
      " 18704 18705 18706 18707 18708 18709 18710 18711 18712 18713 18714 18715\n",
      " 18716 18717 18718 18719 18720 18721 18722 18723 18724 18725 21313 21314\n",
      " 21315 21316 21317 21318 21319 21320 21321 21322 21323 21324 21325 21326\n",
      " 21327 21328 21329 21330 21331 21332 21333 21334 21335 21336 21337 21338\n",
      " 21339 21340 21341 21342 21343 21344 21345 21346 21347 21348 21349 21350\n",
      " 21351 21352 21353 21354 21355 21356 21357 21358 21359 21360 21361 21362\n",
      " 21363 21364 21365 21366 21367 21368 21369 21370 21371 21372 21373 21374\n",
      " 21375 21376 21377 21378 21379 21380 21381 21382 21383 21384 21385 21386\n",
      " 21387 21388 21389 21390 21391 21392 21393 21394 21395 21396 21397 21398\n",
      " 21399 21400 21401 21402 21403 21404 21405 21406 21905 21906 21907 21908\n",
      " 21909 21910 21911 21912 21913 21914 21915 21916 21917 21918 21919 21920\n",
      " 21921 21922 21923 21924 21925 21926 21927 21928 21929 21930 21931 21932\n",
      " 21933 21934 21935 21936 21937 21938 21939 21940 21941 21942 21943 21944\n",
      " 21945 21946 21947 21948 21949 21950 21951 21952 21953 21954 21955 21956\n",
      " 21957 21958 21959 21960 21961 21962 21963 21964 21965 21966 21967 21968\n",
      " 22349 22350 22351 22352 22353 22354 22355 22356 22357 22358 22359 22360\n",
      " 22361 22362 22363 22364 22365 22366 22367 22368 22369 22370 22371 22372\n",
      " 22373 22374 22375 22376 22377 22378 22379 22380 22381 22382 22383 22384\n",
      " 22385 22386 22387 22388 22389 22390 22391 22392 22393 22394 22395 22396\n",
      " 22397 22398 22399 22400 22401 22402 22403 22404 22405 22406 22407 22408\n",
      " 22409 22410 22411 22412 22413 22414 22415 22416 22417 22418 22419 22420\n",
      " 22421 22422 22423 22424 22425 22426 22427 22428 22429 22430 22431 22432\n",
      " 22433 22434 22435 22436 22437 22438 22439 22440 23237 23238 23239 23240\n",
      " 23241 23242 23243 23244 23245 23246 23247 23248 23249 23250 23251 23252\n",
      " 23253 23254 23255 23256 23257 23258 23259 23260 23261 23262 23263 23264\n",
      " 23265 23266 23267 23268 23269 23270 23271 23272 23273 23274 23275 23276\n",
      " 23277 23278 23279 23280 23281 23282 23283 23284 23285 23286 23287 23288\n",
      " 23289 23290 23291 23292 23293 23294 23295 23296 23297 23298 23299 23300\n",
      " 23301 23302 23303 23304 23305 23306 23307 23308 23309 23310 23311 23312\n",
      " 23313 23314 23315 23316 24717 24718 24719 24720 24721 24722 24723 24724\n",
      " 24725 24726 24727 24728 24729 24730 24731 24732 24733 24734 24735 24736\n",
      " 24737 24738 24739 24740 24741 24742 24743 24744 24745 24746 24747 24748\n",
      " 24749 24750 24751 24752 24753 24754 24755 24756 24757 24758 24759 24760\n",
      " 24761 24762 24763 24764 24765 24766 24767 24768 24769 24770 24771 24772\n",
      " 24773 24774 24775 24776 24777 24778 24779 24780 24781 24782 24783 24784\n",
      " 24785 24786 24787 24788 24789 24790 24791 24792 24793 24794 60829 60830\n",
      " 60831 60832 60833 60834 60835 60836 60837 60838 60839 60840 60841 60842\n",
      " 60843 60844 60845 60846 60847 60848 60849 60850 60851 60852 60853 60854\n",
      " 60855 60856 60857 60858 60859 60860 60861 60862 60863 60864 60865 60866\n",
      " 60867 60868 60869 60870 60871 60872 60873 60874 60875 60876 60877 60878\n",
      " 60879 60880 60881 60882 60883 60884 60885 60886 60887 60888 60889 60890\n",
      " 60891 60892 60893 60894 60895 60896 60897 60898 60899 60900 60901 60902\n",
      " 60903 60904 60905 60906 60907 60908 60909 60910 60911 60912 60913 60914\n",
      " 60915 60916 60917 60918 60919 60920 60921 60922 60923 60924 60925 60926\n",
      " 60927 60928 60929 60930 60931 60932 60933 60934 60935 60936 60937 60938\n",
      " 60939 60940 60941 60942 60943 60944 60945 60946 60947 60948 60949 60950\n",
      " 60951 60952 60953 60954 60955 60956 60957 60958 60959 60960 60961 60962\n",
      " 60963 60964 60965 60966 60967 60968 60969 60970 60971 60972 60973 60974\n",
      " 60975 60976 63789 63790 63791 63792 63793 63794 63795 63796 63797 63798\n",
      " 63799 63800 63801 63802 63803 63804 63805 63806 63807 63808 63809 63810\n",
      " 63811 63812 63813 63814 63815 63816 63817 63818 63819 63820 63821 63822\n",
      " 63823 63824 63825 63826 63827 63828 63829 63830 63831 63832 63833 63834\n",
      " 63835 63836 63837 63838 63839 63840 63841 63842 63843 63844 63845 63846\n",
      " 63847 63848 63849 63850 63851 63852 63853 63854 63855 63856 63857 63858\n",
      " 63859 63860 63861 63862 63863 63864 63865 63866 63867 63868 63869 63870\n",
      " 63871 63872 63873 63874 63875 63876 63877 63878 63879 63880 63881 63882\n",
      " 63883 63884 63885 63886 63887 63888 63889 63890 63891 63892 63893 63894\n",
      " 63895 63896 63897 63898 63899]\n"
     ]
    }
   ],
   "source": [
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_threshold = 15840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits with threshold 631\n"
     ]
    }
   ],
   "source": [
    "hits_total = mc_hits[mc_hits[:,13] < event_threshold]\n",
    "print(\"Total hits with threshold %d\" % (len(hits_total)))\n",
    "event = 0\n",
    "idx = {}\n",
    "actual = 0\n",
    "for hit in hits_total:\n",
    "    if (not hit[13] in idx):\n",
    "        p = index.Property()\n",
    "        p.dimension = 3\n",
    "        idx[hit[13]] = index.Index(properties=p, interleaved=False)\n",
    "    \n",
    "    if (len(hit[5]) > 1):\n",
    "        for i in range(0,len(hit[5])):\n",
    "            idx[hit[13]].insert(int(hit[4][i]), (float(hit[5][i]), float(hit[5][i]), hit[8][i], hit[8][i], hit[12][i], hit[12][i]), float(hit[5][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean found 0.856793\n",
      "Total 126125\n"
     ]
    }
   ],
   "source": [
    "X_classifier = []\n",
    "Y_classifier = []\n",
    "\n",
    "total_found = 0\n",
    "total = 0\n",
    "long_range = 300\n",
    "\n",
    "for particle in hits_total:\n",
    "    \n",
    "    if (len(particle[5]) > 1):\n",
    "        \n",
    "        found_bool = 0\n",
    "        found_layers = np.zeros(4)\n",
    "        # Extract only 12 hits, as some of them are really close\n",
    "        zone = particle[12][0]\n",
    "        X_final = {}\n",
    "        actual = -1\n",
    "        \n",
    "        for i in range(0, len(particle[5])):\n",
    "            if (actual != particle[8][i]):\n",
    "                X_final[particle[8][i]] = []\n",
    "                X_final[particle[8][i]].append(particle[5][i])\n",
    "                actual = particle[8][i]\n",
    "            else:\n",
    "                X_final[actual].append(particle[5][i])\n",
    "                \n",
    "        if (not 0 in X_final):\n",
    "            continue\n",
    "            \n",
    "        found_layers[0] = X_final[0][0]\n",
    "            \n",
    "        predicted = (xlayer_model.predict(np.array([X_final[0][0], 0]).reshape(1,-1)))[0]\n",
    "        \n",
    "        if (3 in X_final):\n",
    "            found = list(idx[particle[13]].intersection((predicted[2] - long_range, predicted[2] + long_range, 3, 3, zone, zone),objects='raw'))\n",
    "            found_layer3 = sorted(found, key=lambda x: np.abs(x - predicted[2]))[:20]\n",
    "\n",
    "            for i in range(0,len(found_layer3)):\n",
    "                if (found_layer3[i] in X_final[3]):\n",
    "                    found_bool += 1\n",
    "                    found_layers[3] = found_layer3[i]\n",
    "                    break\n",
    "            \n",
    "        if (2 in X_final):\n",
    "            found = list(idx[particle[13]].intersection((predicted[1] - long_range, predicted[1] + long_range, 2, 2, zone,zone),objects='raw'))\n",
    "            found_layer2 = sorted(found, key=lambda x: np.abs(x - predicted[1]))[:20]\n",
    "\n",
    "            for i in range(0,len(found_layer2)):\n",
    "                if (found_layer2[i] in X_final[2]):\n",
    "                    found_bool += 1\n",
    "                    found_layers[2] = found_layer2[i]\n",
    "                    break\n",
    "        \n",
    "        if (1 in X_final):\n",
    "            found = list(idx[particle[13]].intersection((predicted[0] - long_range, predicted[0] + long_range, 1, 1,zone,zone),objects='raw'))\n",
    "            found_layer1 = sorted(found, key=lambda x: np.abs(x - predicted[0]))[:20]\n",
    "\n",
    "            for i in range(0,len(found_layer1)):\n",
    "                if (found_layer1[i] in X_final[1]):\n",
    "                    found_bool += 1\n",
    "                    found_layers[1] = found_layer1[i]\n",
    "                    break\n",
    "            \n",
    "        if (found_bool == 3):\n",
    "            fake_track = np.array([X_final[0][0], random.choice(found_layer1),random.choice(found_layer2),random.choice(found_layer3)])\n",
    "            true_track = found_layers\n",
    "            \n",
    "            retries = 0\n",
    "            \n",
    "            while (fake_track == true_track).all() and retries != 4:\n",
    "                fake_track = np.array([X_final[0][0], random.choice(found_layer1),random.choice(found_layer2),random.choice(found_layer3)])\n",
    "                retries += 1\n",
    "                \n",
    "            total_found += 1\n",
    "            \n",
    "            if (retries == 4):\n",
    "                X_classifier.append(true_track)\n",
    "                Y_classifier.append(1)\n",
    "            else:\n",
    "                X_classifier.append(true_track)\n",
    "                Y_classifier.append(1)\n",
    "                X_classifier.append(fake_track)\n",
    "                Y_classifier.append(0)\n",
    "            \n",
    "        total += 1\n",
    "\n",
    "print(\"Mean found %f\" % (total_found/total))\n",
    "print(\"Total %d\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_classifier.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_classifier = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "track_classifier.add(Dense(12, activation='relu', input_dim=4))\n",
    "track_classifier.add(Dense(8, activation='relu'))\n",
    "track_classifier.add(Dense(1, activation='sigmoid'))\n",
    "track_classifier.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train length: 161922\n",
      "X Test Length 53974\n",
      "Y Train length: 161922\n",
      "Y Test length: 53974\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X_classifier), np.array(Y_classifier))\n",
    "print(\"X Train length: %d\" % len(X_train))\n",
    "print(\"X Test Length %d\" % len(X_test))\n",
    "print(\"Y Train length: %d\" % len(y_train))\n",
    "print(\"Y Test length: %d\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161922/161922 [==============================] - 2s 14us/sample - loss: 1.7536 - acc: 0.7997\n",
      "Epoch 2/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.2828 - acc: 0.9119\n",
      "Epoch 3/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.2268 - acc: 0.9281\n",
      "Epoch 4/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.2210 - acc: 0.9354\n",
      "Epoch 5/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.1864 - acc: 0.9431\n",
      "Epoch 6/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.1853 - acc: 0.9448\n",
      "Epoch 7/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.1852 - acc: 0.9460\n",
      "Epoch 8/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.1915 - acc: 0.9456\n",
      "Epoch 9/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.1794 - acc: 0.9484\n",
      "Epoch 10/10\n",
      "161922/161922 [==============================] - 2s 13us/sample - loss: 0.1679 - acc: 0.9519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff258b30fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_classifier.fit(X_train, y_train, epochs=10, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53974/53974 [==============================] - 2s 43us/sample - loss: 0.1333 - acc: 0.9594\n",
      "Test score: 0.13332966707968516\n",
      "Test accuracy: 0.9594064\n"
     ]
    }
   ],
   "source": [
    "score, acc = track_classifier.evaluate(X_test, y_test, batch_size=25)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_classifier.save(\"./models/track_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.002824858757062147\n",
      "0.002824858757062147\n",
      "0.005649717514124294\n",
      "0.005649717514124294\n",
      "0.00847457627118644\n",
      "0.00847457627118644\n",
      "0.00847457627118644\n",
      "0.011299435028248588\n",
      "0.00847457627118644\n",
      "0.014124293785310734\n",
      "0.011299435028248588\n",
      "0.01694915254237288\n",
      "0.014124293785310734\n",
      "0.01977401129943503\n",
      "0.014124293785310734\n",
      "0.022598870056497175\n",
      "0.01694915254237288\n",
      "0.025423728813559324\n",
      "0.01977401129943503\n",
      "0.02824858757062147\n",
      "0.022598870056497175\n",
      "0.031073446327683617\n",
      "0.025423728813559324\n",
      "0.03389830508474576\n",
      "0.02824858757062147\n",
      "0.03672316384180791\n",
      "0.031073446327683617\n",
      "0.03954802259887006\n",
      "0.031073446327683617\n",
      "0.0423728813559322\n",
      "0.031073446327683617\n",
      "0.04519774011299435\n",
      "0.03389830508474576\n",
      "0.0480225988700565\n",
      "0.03672316384180791\n",
      "0.05084745762711865\n",
      "0.03954802259887006\n",
      "0.05367231638418079\n",
      "0.0423728813559322\n",
      "0.05649717514124294\n",
      "0.0423728813559322\n",
      "0.059322033898305086\n",
      "0.04519774011299435\n",
      "0.062146892655367235\n",
      "0.0480225988700565\n",
      "0.06779661016949153\n",
      "0.05084745762711865\n",
      "0.07062146892655367\n",
      "0.05367231638418079\n",
      "0.07344632768361582\n",
      "0.05649717514124294\n",
      "0.07627118644067797\n",
      "0.059322033898305086\n",
      "0.07909604519774012\n",
      "0.062146892655367235\n",
      "0.08192090395480225\n",
      "0.06497175141242938\n",
      "0.0847457627118644\n",
      "0.06779661016949153\n",
      "0.0903954802259887\n",
      "0.07062146892655367\n",
      "0.09322033898305085\n",
      "0.07344632768361582\n",
      "0.096045197740113\n",
      "0.07344632768361582\n",
      "0.09887005649717515\n",
      "0.07627118644067797\n",
      "0.1016949152542373\n",
      "0.07909604519774012\n",
      "0.10451977401129943\n",
      "0.08192090395480225\n",
      "0.10734463276836158\n",
      "0.08192090395480225\n",
      "0.11016949152542373\n",
      "0.0847457627118644\n",
      "0.11299435028248588\n",
      "0.0847457627118644\n",
      "0.11581920903954802\n",
      "0.08757062146892655\n",
      "0.11864406779661017\n",
      "0.0903954802259887\n",
      "0.12146892655367232\n",
      "0.0903954802259887\n",
      "0.1271186440677966\n",
      "0.0903954802259887\n",
      "0.12994350282485875\n",
      "0.09322033898305085\n",
      "0.1327683615819209\n",
      "0.096045197740113\n",
      "0.13559322033898305\n",
      "0.09887005649717515\n",
      "0.1384180790960452\n",
      "0.1016949152542373\n",
      "0.14124293785310735\n",
      "0.10451977401129943\n",
      "0.1440677966101695\n",
      "0.10734463276836158\n",
      "0.14689265536723164\n",
      "0.10734463276836158\n",
      "0.1497175141242938\n",
      "0.10734463276836158\n",
      "0.15254237288135594\n",
      "0.11016949152542373\n",
      "0.1553672316384181\n",
      "0.11299435028248588\n",
      "0.15819209039548024\n",
      "0.11581920903954802\n",
      "0.16101694915254236\n",
      "0.11864406779661017\n",
      "0.1638418079096045\n",
      "0.12146892655367232\n",
      "0.16666666666666666\n",
      "0.12429378531073447\n",
      "0.1694915254237288\n",
      "0.1271186440677966\n",
      "0.17231638418079095\n",
      "0.12994350282485875\n",
      "0.1751412429378531\n",
      "0.1327683615819209\n",
      "0.17796610169491525\n",
      "0.13559322033898305\n",
      "0.18361581920903955\n",
      "0.1384180790960452\n",
      "0.1864406779661017\n",
      "0.1384180790960452\n",
      "0.18926553672316385\n",
      "0.14124293785310735\n",
      "0.192090395480226\n",
      "0.14124293785310735\n",
      "0.19491525423728814\n",
      "0.1440677966101695\n",
      "0.1977401129943503\n",
      "0.14689265536723164\n",
      "0.20056497175141244\n",
      "0.14689265536723164\n",
      "0.2033898305084746\n",
      "0.1497175141242938\n",
      "0.2062146892655367\n",
      "0.15254237288135594\n",
      "0.20903954802259886\n",
      "0.15254237288135594\n",
      "0.211864406779661\n",
      "0.15254237288135594\n",
      "0.21468926553672316\n",
      "0.1553672316384181\n",
      "0.2175141242937853\n",
      "0.15819209039548024\n",
      "0.22033898305084745\n",
      "0.15819209039548024\n",
      "0.2231638418079096\n",
      "0.16101694915254236\n",
      "0.22598870056497175\n",
      "0.1638418079096045\n",
      "0.2288135593220339\n",
      "0.16666666666666666\n",
      "0.23163841807909605\n",
      "0.1694915254237288\n",
      "0.2344632768361582\n",
      "0.1694915254237288\n",
      "0.23728813559322035\n",
      "0.17231638418079095\n",
      "0.2401129943502825\n",
      "0.17231638418079095\n",
      "0.24293785310734464\n",
      "0.1751412429378531\n",
      "0.2457627118644068\n",
      "0.17796610169491525\n",
      "0.24858757062146894\n",
      "0.17796610169491525\n",
      "0.2514124293785311\n",
      "0.1807909604519774\n",
      "0.2542372881355932\n",
      "0.18361581920903955\n",
      "0.2570621468926554\n",
      "0.1864406779661017\n",
      "0.2598870056497175\n",
      "0.18926553672316385\n",
      "0.2627118644067797\n",
      "0.18926553672316385\n",
      "0.2655367231638418\n",
      "0.18926553672316385\n",
      "0.268361581920904\n",
      "0.18926553672316385\n",
      "0.2740112994350282\n",
      "0.192090395480226\n",
      "0.2768361581920904\n",
      "0.192090395480226\n",
      "0.2796610169491525\n",
      "0.19491525423728814\n",
      "0.2824858757062147\n",
      "0.1977401129943503\n",
      "0.2853107344632768\n",
      "0.1977401129943503\n",
      "0.288135593220339\n",
      "0.1977401129943503\n",
      "0.2909604519774011\n",
      "0.20056497175141244\n",
      "0.2937853107344633\n",
      "0.2033898305084746\n",
      "0.2966101694915254\n",
      "0.2033898305084746\n",
      "0.2994350282485876\n",
      "0.2033898305084746\n",
      "0.3022598870056497\n",
      "0.2033898305084746\n",
      "0.3050847457627119\n",
      "0.2062146892655367\n",
      "0.307909604519774\n",
      "0.2062146892655367\n",
      "0.3107344632768362\n",
      "0.2062146892655367\n",
      "0.3135593220338983\n",
      "0.20903954802259886\n",
      "0.3163841807909605\n",
      "0.211864406779661\n",
      "0.3192090395480226\n",
      "0.21468926553672316\n",
      "0.3220338983050847\n",
      "0.2175141242937853\n",
      "0.3248587570621469\n",
      "0.22033898305084745\n",
      "0.3305084745762712\n",
      "0.22033898305084745\n",
      "0.3361581920903955\n",
      "0.2231638418079096\n",
      "0.3389830508474576\n",
      "0.22598870056497175\n",
      "0.3418079096045198\n",
      "0.22598870056497175\n",
      "0.3446327683615819\n",
      "0.22598870056497175\n",
      "0.3474576271186441\n",
      "0.2288135593220339\n",
      "0.3502824858757062\n",
      "0.23163841807909605\n",
      "0.3531073446327684\n",
      "0.23163841807909605\n",
      "0.3559322033898305\n",
      "0.2344632768361582\n",
      "0.3587570621468927\n",
      "0.23728813559322035\n",
      "0.3615819209039548\n",
      "0.23728813559322035\n",
      "0.3644067796610169\n",
      "0.23728813559322035\n",
      "0.3672316384180791\n",
      "0.2401129943502825\n",
      "0.3700564971751412\n",
      "0.2401129943502825\n",
      "0.3728813559322034\n",
      "0.24293785310734464\n",
      "0.3757062146892655\n",
      "0.2457627118644068\n",
      "0.3785310734463277\n",
      "0.24858757062146894\n",
      "0.3813559322033898\n",
      "0.2514124293785311\n",
      "0.384180790960452\n",
      "0.2542372881355932\n",
      "0.3870056497175141\n",
      "0.2570621468926554\n",
      "0.3898305084745763\n",
      "0.2598870056497175\n",
      "0.3926553672316384\n",
      "0.2598870056497175\n",
      "0.3954802259887006\n",
      "0.2598870056497175\n",
      "0.3983050847457627\n",
      "0.2598870056497175\n",
      "0.4011299435028249\n",
      "0.2598870056497175\n",
      "0.403954802259887\n",
      "0.2627118644067797\n",
      "0.4067796610169492\n",
      "0.2655367231638418\n",
      "0.4096045197740113\n",
      "0.268361581920904\n",
      "0.4124293785310734\n",
      "0.268361581920904\n",
      "0.4152542372881356\n",
      "0.268361581920904\n",
      "0.4180790960451977\n",
      "0.2711864406779661\n",
      "0.4209039548022599\n",
      "0.2740112994350282\n",
      "0.423728813559322\n",
      "0.2740112994350282\n",
      "0.4265536723163842\n",
      "0.2768361581920904\n",
      "0.4293785310734463\n",
      "0.2796610169491525\n",
      "0.4322033898305085\n",
      "0.2824858757062147\n",
      "0.4378531073446328\n",
      "0.2824858757062147\n",
      "0.4406779661016949\n",
      "0.2853107344632768\n",
      "0.4463276836158192\n",
      "0.288135593220339\n",
      "0.4491525423728814\n",
      "0.2909604519774011\n",
      "0.4519774011299435\n",
      "0.2937853107344633\n",
      "0.4576271186440678\n",
      "0.2966101694915254\n",
      "0.4604519774011299\n",
      "0.2966101694915254\n",
      "0.4632768361581921\n",
      "0.2994350282485876\n",
      "0.4661016949152542\n",
      "0.2994350282485876\n",
      "0.4689265536723164\n",
      "0.2994350282485876\n",
      "0.4717514124293785\n",
      "0.3022598870056497\n",
      "0.4745762711864407\n",
      "0.3050847457627119\n",
      "0.4774011299435028\n",
      "0.307909604519774\n",
      "0.4830508474576271\n",
      "0.3107344632768362\n",
      "0.4858757062146893\n",
      "0.3135593220338983\n",
      "0.4887005649717514\n",
      "0.3163841807909605\n",
      "0.4915254237288136\n",
      "0.3192090395480226\n",
      "0.4943502824858757\n",
      "0.3220338983050847\n",
      "0.4971751412429379\n",
      "0.3248587570621469\n",
      "0.5\n",
      "0.327683615819209\n",
      "0.5028248587570622\n",
      "0.3305084745762712\n",
      "0.5056497175141242\n",
      "0.3333333333333333\n",
      "0.5084745762711864\n",
      "0.3361581920903955\n",
      "0.5112994350282486\n",
      "0.3361581920903955\n",
      "0.5141242937853108\n",
      "0.3389830508474576\n",
      "0.5169491525423728\n",
      "0.3418079096045198\n",
      "0.519774011299435\n",
      "0.3446327683615819\n",
      "0.5225988700564972\n",
      "0.3446327683615819\n",
      "0.5254237288135594\n",
      "0.3474576271186441\n",
      "0.5282485875706214\n",
      "0.3502824858757062\n",
      "0.5310734463276836\n",
      "0.3531073446327684\n",
      "0.5338983050847458\n",
      "0.3531073446327684\n",
      "0.536723163841808\n",
      "0.3559322033898305\n",
      "0.53954802259887\n",
      "0.3587570621468927\n",
      "0.5423728813559322\n",
      "0.3615819209039548\n",
      "0.5451977401129944\n",
      "0.3644067796610169\n",
      "0.5480225988700564\n",
      "0.3672316384180791\n",
      "0.5508474576271186\n",
      "0.3672316384180791\n",
      "0.5536723163841808\n",
      "0.3700564971751412\n",
      "0.556497175141243\n",
      "0.3700564971751412\n",
      "0.559322033898305\n",
      "0.3728813559322034\n",
      "0.5621468926553672\n",
      "0.3757062146892655\n",
      "0.5649717514124294\n",
      "0.3785310734463277\n",
      "0.5677966101694916\n",
      "0.3813559322033898\n",
      "0.5706214689265536\n",
      "0.384180790960452\n",
      "0.5734463276836158\n",
      "0.384180790960452\n",
      "0.576271186440678\n",
      "0.3870056497175141\n",
      "0.5790960451977402\n",
      "0.3898305084745763\n",
      "0.5819209039548022\n",
      "0.3926553672316384\n",
      "0.5847457627118644\n",
      "0.3954802259887006\n",
      "0.5875706214689266\n",
      "0.3983050847457627\n",
      "0.5903954802259888\n",
      "0.4011299435028249\n",
      "0.5932203389830508\n",
      "0.4011299435028249\n",
      "0.596045197740113\n",
      "0.403954802259887\n",
      "0.5988700564971752\n",
      "0.4067796610169492\n",
      "0.6016949152542372\n",
      "0.4067796610169492\n",
      "0.6045197740112994\n",
      "0.4067796610169492\n",
      "0.6129943502824858\n",
      "0.4067796610169492\n",
      "0.615819209039548\n",
      "0.4096045197740113\n",
      "0.6186440677966102\n",
      "0.4096045197740113\n",
      "0.6214689265536724\n",
      "0.4124293785310734\n",
      "0.6242937853107344\n",
      "0.4124293785310734\n",
      "0.6271186440677966\n",
      "0.4124293785310734\n",
      "0.6299435028248588\n",
      "0.4124293785310734\n",
      "0.632768361581921\n",
      "0.4124293785310734\n",
      "0.635593220338983\n",
      "0.4124293785310734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6412429378531074\n",
      "0.4124293785310734\n",
      "0.6440677966101694\n",
      "0.4124293785310734\n",
      "0.6468926553672316\n",
      "0.4152542372881356\n",
      "0.6497175141242938\n",
      "0.4180790960451977\n",
      "0.652542372881356\n",
      "0.4180790960451977\n",
      "0.6581920903954802\n",
      "0.4180790960451977\n",
      "0.6638418079096046\n",
      "0.4209039548022599\n",
      "0.6666666666666666\n",
      "0.4209039548022599\n",
      "0.6694915254237288\n",
      "0.423728813559322\n",
      "0.672316384180791\n",
      "0.4265536723163842\n",
      "0.6751412429378532\n",
      "0.4293785310734463\n",
      "0.6779661016949152\n",
      "0.4322033898305085\n",
      "0.6807909604519774\n",
      "0.4322033898305085\n",
      "0.6836158192090396\n",
      "0.4350282485875706\n",
      "0.6864406779661016\n",
      "0.4350282485875706\n",
      "0.6892655367231638\n",
      "0.4378531073446328\n",
      "0.692090395480226\n",
      "0.4406779661016949\n",
      "0.6949152542372882\n",
      "0.4435028248587571\n",
      "0.6977401129943502\n",
      "0.4463276836158192\n",
      "0.7005649717514124\n",
      "0.4491525423728814\n",
      "0.7033898305084746\n",
      "0.4519774011299435\n",
      "0.7062146892655368\n",
      "0.4519774011299435\n",
      "0.7090395480225988\n",
      "0.4548022598870056\n",
      "0.711864406779661\n",
      "0.4548022598870056\n",
      "0.7146892655367232\n",
      "0.4548022598870056\n",
      "0.7175141242937854\n",
      "0.4548022598870056\n",
      "0.7231638418079096\n",
      "0.4576271186440678\n",
      "0.7259887005649718\n",
      "0.4576271186440678\n",
      "0.7288135593220338\n",
      "0.4604519774011299\n",
      "0.731638418079096\n",
      "0.4604519774011299\n",
      "0.7344632768361582\n",
      "0.4632768361581921\n",
      "0.7372881355932204\n",
      "0.4661016949152542\n",
      "0.7401129943502824\n",
      "0.4689265536723164\n",
      "0.7429378531073446\n",
      "0.4689265536723164\n",
      "0.748587570621469\n",
      "0.4717514124293785\n",
      "0.751412429378531\n",
      "0.4717514124293785\n",
      "0.7542372881355932\n",
      "0.4717514124293785\n",
      "0.7570621468926554\n",
      "0.4717514124293785\n",
      "0.7598870056497176\n",
      "0.4745762711864407\n",
      "0.7627118644067796\n",
      "0.4774011299435028\n",
      "0.7655367231638418\n",
      "0.480225988700565\n",
      "0.768361581920904\n",
      "0.480225988700565\n",
      "0.7740112994350282\n",
      "0.4830508474576271\n",
      "0.7768361581920904\n",
      "0.4830508474576271\n",
      "0.7796610169491526\n",
      "0.4830508474576271\n",
      "0.7824858757062146\n",
      "0.4830508474576271\n",
      "0.7853107344632768\n",
      "0.4830508474576271\n",
      "0.788135593220339\n",
      "0.4858757062146893\n",
      "0.7909604519774012\n",
      "0.4887005649717514\n",
      "0.7937853107344632\n",
      "0.4915254237288136\n",
      "0.7966101694915254\n",
      "0.4943502824858757\n",
      "0.7994350282485876\n",
      "0.4971751412429379\n",
      "0.8022598870056498\n",
      "0.5\n",
      "0.8050847457627118\n",
      "0.5028248587570622\n",
      "0.807909604519774\n",
      "0.5056497175141242\n",
      "0.8107344632768362\n",
      "0.5084745762711864\n",
      "0.8135593220338984\n",
      "0.5084745762711864\n",
      "0.8163841807909604\n",
      "0.5112994350282486\n",
      "0.8192090395480226\n",
      "0.5112994350282486\n",
      "0.8248587570621468\n",
      "0.5112994350282486\n",
      "0.827683615819209\n",
      "0.5112994350282486\n",
      "0.8305084745762712\n",
      "0.5112994350282486\n",
      "0.8333333333333334\n",
      "0.5112994350282486\n",
      "0.8361581920903954\n",
      "0.5112994350282486\n",
      "0.8389830508474576\n",
      "0.5141242937853108\n",
      "0.844632768361582\n",
      "0.5141242937853108\n",
      "0.847457627118644\n",
      "0.5169491525423728\n",
      "0.8502824858757062\n",
      "0.5169491525423728\n",
      "0.8531073446327684\n",
      "0.519774011299435\n",
      "0.8559322033898306\n",
      "0.5225988700564972\n",
      "0.8587570621468926\n",
      "0.5254237288135594\n",
      "0.8615819209039548\n",
      "0.5282485875706214\n",
      "0.864406779661017\n",
      "0.5282485875706214\n",
      "0.867231638418079\n",
      "0.5282485875706214\n",
      "0.8700564971751412\n",
      "0.5282485875706214\n",
      "0.8728813559322034\n",
      "0.5310734463276836\n",
      "0.8785310734463276\n",
      "0.5310734463276836\n",
      "0.8813559322033898\n",
      "0.5338983050847458\n",
      "0.884180790960452\n",
      "0.5338983050847458\n",
      "0.8870056497175142\n",
      "0.536723163841808\n",
      "0.8898305084745762\n",
      "0.53954802259887\n",
      "0.8926553672316384\n",
      "0.53954802259887\n",
      "0.8954802259887006\n",
      "0.53954802259887\n",
      "0.8983050847457628\n",
      "0.5423728813559322\n",
      "0.9011299435028248\n",
      "0.5451977401129944\n",
      "0.903954802259887\n",
      "0.5451977401129944\n",
      "0.9067796610169492\n",
      "0.5451977401129944\n",
      "0.9152542372881356\n",
      "0.5451977401129944\n",
      "0.9180790960451978\n",
      "0.5480225988700564\n",
      "0.9209039548022598\n",
      "0.5480225988700564\n",
      "0.923728813559322\n",
      "0.5508474576271186\n",
      "0.9265536723163842\n",
      "0.5536723163841808\n",
      "0.9293785310734464\n",
      "0.556497175141243\n",
      "0.9322033898305084\n",
      "0.559322033898305\n",
      "0.9350282485875706\n",
      "0.5621468926553672\n",
      "0.9378531073446328\n",
      "0.5649717514124294\n",
      "0.943502824858757\n",
      "0.5677966101694916\n",
      "0.9463276836158192\n",
      "0.5706214689265536\n",
      "0.9491525423728814\n",
      "0.5706214689265536\n",
      "0.9548022598870056\n",
      "0.5706214689265536\n",
      "0.96045197740113\n",
      "0.5734463276836158\n",
      "0.963276836158192\n",
      "0.5734463276836158\n",
      "0.9661016949152542\n",
      "0.5734463276836158\n",
      "0.9689265536723164\n",
      "0.576271186440678\n",
      "0.9717514124293786\n",
      "0.5790960451977402\n",
      "0.9745762711864406\n",
      "0.5819209039548022\n",
      "0.9774011299435028\n",
      "0.5819209039548022\n",
      "0.980225988700565\n",
      "0.5819209039548022\n",
      "0.9830508474576272\n",
      "0.5847457627118644\n",
      "0.9858757062146892\n",
      "0.5847457627118644\n",
      "0.9887005649717514\n",
      "0.5847457627118644\n",
      "0.9915254237288136\n",
      "0.5875706214689266\n",
      "0.9971751412429378\n",
      "0.5903954802259888\n",
      "Mean found 0.590395\n",
      "Total 354\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "X_test = []\n",
    "\n",
    "total_found = 0\n",
    "total = 0\n",
    "long_range = 300\n",
    "\n",
    "hits_total = mc_hits[mc_hits[:,13] < event_threshold]\n",
    "\n",
    "for particle in hits_total:\n",
    "    \n",
    "    if (len(particle[5]) > 1):\n",
    "        \n",
    "        found_bool = 0\n",
    "        found_layers = np.zeros(3)\n",
    "        # Extract only 12 hits, as some of them are really close\n",
    "        zone = particle[12][0]\n",
    "        X_final = {}\n",
    "        actual = -1\n",
    "        \n",
    "        for i in range(0, len(particle[5])):\n",
    "            if (actual != particle[8][i]):\n",
    "                X_final[particle[8][i]] = []\n",
    "                X_final[particle[8][i]].append(particle[5][i])\n",
    "                actual = particle[8][i]\n",
    "            else:\n",
    "                X_final[actual].append(particle[5][i])\n",
    "                \n",
    "        if (not 0 in X_final):\n",
    "            continue\n",
    "            \n",
    "        predicted = (xlayer_model.predict(np.array([X_final[0][0], 0]).reshape(1,-1)))[0]\n",
    "        \n",
    "        if (3 in X_final):\n",
    "            found = list(idx[particle[13]].intersection((predicted[2] - long_range, predicted[2] + long_range, 3, 3, zone, zone),objects='raw'))\n",
    "            found_layer3 = sorted(found, key=lambda x: np.abs(x - predicted[2]))[:10]\n",
    "            found_bool += 1\n",
    "            \n",
    "        if (2 in X_final):\n",
    "            found = list(idx[particle[13]].intersection((predicted[1] - long_range, predicted[1] + long_range, 2, 2, zone,zone),objects='raw'))\n",
    "            found_layer2 = sorted(found, key=lambda x: np.abs(x - predicted[1]))[:10]\n",
    "            found_bool += 1\n",
    "        \n",
    "        if (1 in X_final):\n",
    "            found = list(idx[particle[13]].intersection((predicted[0] - long_range, predicted[0] + long_range, 1, 1,zone,zone),objects='raw'))\n",
    "            found_layer1 = sorted(found, key=lambda x: np.abs(x - predicted[0]))[:10]\n",
    "            found_bool += 1\n",
    "            \n",
    "        if (found_bool == 3):\n",
    "            retries = 0\n",
    "            \n",
    "            valid_list = []\n",
    "            max_val = 0\n",
    "            for h3 in found_layer3[:8]:\n",
    "                for h2 in found_layer2[:8]:\n",
    "                    for h1 in found_layer1[:8]:\n",
    "                        pred = (track_classifier.predict(np.array([X_final[0][0], h1, h2, h3]).reshape(1,-1))[0])[0]\n",
    "                        if pred > max_val:\n",
    "                            valid = [X_final[0][0], h1, h2, h3]\n",
    "                            max_val = pred\n",
    "            \n",
    "            total_val = 0\n",
    "            if valid[1] in X_final[1]:\n",
    "                total_val = 1\n",
    "                \n",
    "            if valid[2] in X_final[2]:\n",
    "                total_val = 1\n",
    "                \n",
    "            if valid[3] in X_final[3]:\n",
    "                total_val = 1\n",
    "                \n",
    "            total_found += total_val\n",
    "                \n",
    "            print(total/354)\n",
    "            print(total_found/354)\n",
    "            #print(total_found/354)\n",
    "            #print(total_val)\n",
    "            #print('\\n')\n",
    "            \n",
    "        total += 1\n",
    "\n",
    "print(\"Mean found %f\" % (total_found/total))\n",
    "print(\"Total %d\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
